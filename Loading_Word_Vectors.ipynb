{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import math\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "Using \"Dependency Based\" dataset from\n",
    "url: https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Code for loading bin file is from a blog post\n",
    "url: https://blog.ekbana.com/loading-glove-pre-trained-word-embedding-model-from-text-file-faster-5d3e8f2b8455\n",
    "\"\"\"\n",
    "def convert_to_binary(embedding_path):\n",
    "    \"\"\"\n",
    "    Here, it takes path to embedding text file provided by glove.\n",
    "    :param embedding_path: takes path of the embedding which is in text format or any format other than binary.\n",
    "    :return: a binary file of the given embeddings which takes a lot less time to load.\n",
    "    \"\"\"\n",
    "    f = codecs.open(embedding_path + \".txt\", 'r', encoding='utf-8')\n",
    "    wv = []\n",
    "    with codecs.open(embedding_path + \".vocab\", \"w\", encoding='utf-8') as vocab_write:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if count == 0:\n",
    "                pass\n",
    "            else:\n",
    "                splitlines = line.split()\n",
    "                vocab_write.write(splitlines[0].strip())\n",
    "                vocab_write.write(\"\\n\")\n",
    "                wv.append([float(val) for val in splitlines[1:]])\n",
    "            count += 1\n",
    "    np.save(embedding_path + \".npy\", np.array(wv))\n",
    "    \n",
    "def load_embeddings_binary(embeddings_path):\n",
    "    \"\"\"\n",
    "    It loads embedding provided by glove which is saved as binary file. Loading of this model is\n",
    "    about  second faster than that of loading of txt glove file as model.\n",
    "    :param embeddings_path: path of glove file.\n",
    "    :return: glove model\n",
    "    \"\"\"\n",
    "    with codecs.open(embeddings_path + '.vocab', 'r', 'utf-8') as f_in:\n",
    "        index2word = [line.strip() for line in f_in]\n",
    "    wv = np.load(embeddings_path + '.npy')\n",
    "    model = {}\n",
    "    for i, w in enumerate(index2word):\n",
    "        model[w] = wv[i]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to find top similar words as defined by dot product\n",
    "Written by us :)\n",
    "\"\"\"\n",
    "def n_similar(inputVec, n, keySpace, model):\n",
    "    topWord = []\n",
    "    topDot = []\n",
    "    for i in range(0,n):\n",
    "        topWord.append('')\n",
    "        topDot.append(0)\n",
    "    length = inputVec.shape[0]\n",
    "    for key in keySpace:\n",
    "        lenKey = (np.reshape(model[key],(1,length)) @ np.reshape(model[key],(length,1))) ** .5\n",
    "        lenInput = (np.reshape(inputVec,(1,length)) @ np.reshape(inputVec,(length,1))) ** .5\n",
    "        dot = np.reshape(inputVec,(1,length)) @ np.reshape(model[key],(length,1)) / lenKey / lenInput\n",
    "        for j in range(0,n):\n",
    "            if (dot > topDot[n - j - 1]):\n",
    "                if (j != 0):\n",
    "                    topWord[n - j] = topWord[n - j - 1]\n",
    "                    topDot[n - j] = topDot[n - j - 1]\n",
    "                topWord[n - j - 1] = key\n",
    "                topDot[n - j - 1] = dot\n",
    "    return topWord, topDot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " try:\n",
    "     path = \"deps\"\n",
    "     x = load_embeddings_binary(path)\n",
    "     keys = x.keys()\n",
    " except FileNotFoundError:\n",
    "     raise Exception(f'FILE {path}.txt NOT FOUND PLEASE DOWNLOAD DEPENDENCY-BASED [WORDS] DATASET AND EXTRACT IN DIRECTORY FROM https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math = x['apple'] - x['fruit']\n",
    "vals = keys\n",
    "result, length = n_similar(math, 5, vals, x)\n",
    "print(result)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the spam training data\n",
    "csvImport = np.loadtxt('spam.csv',delimiter=',', dtype = 'S100',encoding = 'latin-1', usecols = (0,1), skiprows = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxRow = np.shape(csvImport)[0]\n",
    "yTrain = np.zeros((maxRow,1))\n",
    "xTrain = np.zeros((maxRow,300))\n",
    "\n",
    "# get the average word vector for each text\n",
    "for i in range(0,maxRow):\n",
    "    if (str(csvImport[i,0])[2:-1] == \"spam\"):\n",
    "        yTrain[i] = -1\n",
    "    else:\n",
    "        yTrain[i] = 1\n",
    "    \n",
    "    vecSum = np.zeros((300,))\n",
    "    wordCount = 0\n",
    "\n",
    "    words = str(csvImport[i,1])[2:-1].split(' ')\n",
    "    for word in words:\n",
    "        try:\n",
    "            # for every word, if it is in the vocabulary from word2vec, add the word vector to the sum\n",
    "            word.translate(str.maketrans('', '', string.punctuation))\n",
    "            wordVec = x[word]\n",
    "            vecSum = wordVec + vecSum\n",
    "            wordCount = wordCount + 1\n",
    "        except:\n",
    "            pass\n",
    "    if (wordCount > 0):\n",
    "        # average out the words\n",
    "        avgWord = vecSum / wordCount\n",
    "    else:\n",
    "        avgWord - vecSum\n",
    "    xTrain[i,:] = np.reshape(avgWord,(1,300))\n",
    "# xTrain has row vectors of the average word value\n",
    "\n",
    "# create a new vocab from the texts\n",
    "bagVocab = []\n",
    "# get vocab for the bag so it isn't too big\n",
    "for i in range(0,maxRow):\n",
    "    words = str(csvImport[i,1])[2:-1].split(' ')\n",
    "    for word in words:\n",
    "        word.translate(str.maketrans('', '', string.punctuation))\n",
    "        # if the word is in the word2vec vocabulary and not already in our new vocab, put it in the new vocab\n",
    "        if (word in x.keys() and (not word in bagVocab)):\n",
    "            bagVocab.append(word)\n",
    "            \n",
    "# create a new matrix for info from bag of words\n",
    "xTrainBag = np.zeros((maxRow,len(bagVocab)))\n",
    "\n",
    "# get bag of word count vector\n",
    "for i in range(0,maxRow):\n",
    "    words = str(csvImport[i,1])[2:-1].split(' ')\n",
    "    for word in words:\n",
    "        word.translate(str.maketrans('', '', string.punctuation))\n",
    "        try:\n",
    "            # update bag count when a word is found\n",
    "            index = bagVocab.index(word)\n",
    "            xTrainBag[i,index] = xTrainBag[i,index] + 1\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform truncated svd on the average word vector model\n",
    "import math\n",
    "\n",
    "parts = 8\n",
    "\n",
    "split_X = np.array_split(xTrain, parts)\n",
    "split_y = np.array_split(yTrain, parts)\n",
    "\n",
    "# You can update these parameters if you have extra computation time\n",
    "svd_parameters = range(9,10)\n",
    "\n",
    "svd_errors = []\n",
    "\n",
    "def calcErrorCount(expected, actual):\n",
    "    return (expected != actual).sum()\n",
    "\n",
    "def calcErrorRate(expected, actual):\n",
    "    return calcErrorCount(expected, actual) / len(actual)\n",
    "\n",
    "# hard coded so the other test is consistent\n",
    "for errorEstimateIndex in range(0, 1):\n",
    "    for predictIndex in range(1, 2):\n",
    "        if errorEstimateIndex == predictIndex:\n",
    "            continue\n",
    "        \n",
    "        training_X = np.vstack(np.delete(split_X, [errorEstimateIndex, predictIndex], axis=0))\n",
    "        training_y = np.vstack(np.delete(split_y, [errorEstimateIndex, predictIndex], axis=0))\n",
    "        \n",
    "        print(np.shape(training_X))\n",
    "        \n",
    "        error_estimation_X = split_X[errorEstimateIndex]\n",
    "        error_estimation_y = split_y[errorEstimateIndex]\n",
    "        \n",
    "        predict_X = split_X[predictIndex]\n",
    "        predict_y = split_y[predictIndex]\n",
    "        \n",
    "        \"\"\"\n",
    "        Truncated SVD\n",
    "        \"\"\"\n",
    "        \n",
    "        # Estimate w for each choice of the regularization parameter\n",
    "        svd_w = []\n",
    "        for param in svd_parameters:\n",
    "            U,s,VT = np.linalg.svd(training_X,full_matrices=False)\n",
    "    \n",
    "            V_train = VT.T[:,0:param]\n",
    "            s_inv = np.linalg.inv(np.identity(param) * s[0:param])\n",
    "            UT_train = U.T[0:param,:]\n",
    "    \n",
    "            w = V_train @ s_inv @ UT_train @ training_y\n",
    "        \n",
    "            svd_w.append(w)\n",
    "        \n",
    "        # Select the best value for the regularization parameter\n",
    "        # by estimating the error on the first holdout set\n",
    "        \n",
    "        leastErrorRateSvd = math.inf\n",
    "        leastErrorParamSvd = None\n",
    "        \n",
    "        for i in range(len(svd_w)):\n",
    "            w = svd_w[i]\n",
    "            estimated_y = np.sign(error_estimation_X @ w)\n",
    "            errorRate = calcErrorRate(error_estimation_y, estimated_y)\n",
    "            \n",
    "            if errorRate < leastErrorRateSvd:\n",
    "                leastErrorRateSvd = errorRate\n",
    "                leastErrorParamSvd = svd_parameters[i]\n",
    "        \n",
    "        # Use the w corresponding to the best value of the regularization\n",
    "        # parameter to predict the labels of the remaining holdout set\n",
    "        \n",
    "        predicted_y = np.sign(predict_X @ w)\n",
    "        errorRate = calcErrorRate(predict_y, predicted_y)\n",
    "        svd_errors.append(errorRate)\n",
    "\n",
    "print(f'Average SVD Error Rate: {(np.mean(svd_errors) * 100).round(3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = 8\n",
    "\n",
    "split_X = np.array_split(xTrainBag, parts)\n",
    "split_y = np.array_split(yTrain, parts)\n",
    "\n",
    "# You can update these parameters if you have extra computation time\n",
    "# this matrix is a bit large to compute\n",
    "svd_parameters = range(9,10)\n",
    "\n",
    "svd_errors = []\n",
    "\n",
    "def calcErrorCount(expected, actual):\n",
    "    return (expected != actual).sum()\n",
    "\n",
    "def calcErrorRate(expected, actual):\n",
    "    return calcErrorCount(expected, actual) / len(actual)\n",
    "\n",
    "# hardcoded because it takes a while to run\n",
    "for errorEstimateIndex in range(0, 1):\n",
    "    for predictIndex in range(1, 2):\n",
    "        if errorEstimateIndex == predictIndex:\n",
    "            continue\n",
    "        \n",
    "        training_X = np.vstack(np.delete(split_X, [errorEstimateIndex, predictIndex], axis=0))\n",
    "        training_y = np.vstack(np.delete(split_y, [errorEstimateIndex, predictIndex], axis=0))\n",
    "        \n",
    "        print(np.shape(training_X))\n",
    "        \n",
    "        error_estimation_X = split_X[errorEstimateIndex]\n",
    "        error_estimation_y = split_y[errorEstimateIndex]\n",
    "        \n",
    "        predict_X = split_X[predictIndex]\n",
    "        predict_y = split_y[predictIndex]\n",
    "        \n",
    "        \"\"\"\n",
    "        Truncated SVD\n",
    "        \"\"\"\n",
    "        \n",
    "        # Estimate w for each choice of the regularization parameter\n",
    "        svd_w = []\n",
    "        for param in svd_parameters:\n",
    "            U,s,VT = np.linalg.svd(training_X,full_matrices=False)\n",
    "    \n",
    "            V_train = VT.T[:,0:param]\n",
    "            s_inv = np.linalg.inv(np.identity(param) * s[0:param])\n",
    "            UT_train = U.T[0:param,:]\n",
    "    \n",
    "            w = V_train @ s_inv @ UT_train @ training_y\n",
    "        \n",
    "            svd_w.append(w)\n",
    "        \n",
    "        # Select the best value for the regularization parameter\n",
    "        # by estimating the error on the first holdout set\n",
    "        \n",
    "        leastErrorRateSvd = math.inf\n",
    "        leastErrorParamSvd = None\n",
    "        \n",
    "        for i in range(len(svd_w)):\n",
    "            w = svd_w[i]\n",
    "            estimated_y = np.sign(error_estimation_X @ w)\n",
    "            errorRate = calcErrorRate(error_estimation_y, estimated_y)\n",
    "            \n",
    "            if errorRate < leastErrorRateSvd:\n",
    "                leastErrorRateSvd = errorRate\n",
    "                leastErrorParamSvd = svd_parameters[i]\n",
    "\n",
    "        # Use the w corresponding to the best value of the regularization\n",
    "        # parameter to predict the labels of the remaining holdout set\n",
    "        \n",
    "        predicted_y = np.sign(predict_X @ w)\n",
    "        errorRate = calcErrorRate(predict_y, predicted_y)\n",
    "        svd_errors.append(errorRate)\n",
    "\n",
    "print(f'Average SVD Error Rate: {(np.mean(svd_errors) * 100).round(3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
