{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import math\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "Using \"Dependency Based\" dataset from\n",
    "url: https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Code for loading bin file is from a blog post\n",
    "url: https://blog.ekbana.com/loading-glove-pre-trained-word-embedding-model-from-text-file-faster-5d3e8f2b8455\n",
    "\"\"\"\n",
    "def convert_to_binary(embedding_path):\n",
    "    \"\"\"\n",
    "    Here, it takes path to embedding text file provided by glove.\n",
    "    :param embedding_path: takes path of the embedding which is in text format or any format other than binary.\n",
    "    :return: a binary file of the given embeddings which takes a lot less time to load.\n",
    "    \"\"\"\n",
    "    f = codecs.open(embedding_path + \".txt\", 'r', encoding='utf-8')\n",
    "    wv = []\n",
    "    with codecs.open(embedding_path + \".vocab\", \"w\", encoding='utf-8') as vocab_write:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if count == 0:\n",
    "                pass\n",
    "            else:\n",
    "                splitlines = line.split()\n",
    "                vocab_write.write(splitlines[0].strip())\n",
    "                vocab_write.write(\"\\n\")\n",
    "                wv.append([float(val) for val in splitlines[1:]])\n",
    "            count += 1\n",
    "    np.save(embedding_path + \".npy\", np.array(wv))\n",
    "    \n",
    "def load_embeddings_binary(embeddings_path):\n",
    "    \"\"\"\n",
    "    It loads embedding provided by glove which is saved as binary file. Loading of this model is\n",
    "    about  second faster than that of loading of txt glove file as model.\n",
    "    :param embeddings_path: path of glove file.\n",
    "    :return: glove model\n",
    "    \"\"\"\n",
    "    with codecs.open(embeddings_path + '.vocab', 'r', 'utf-8') as f_in:\n",
    "        index2word = [line.strip() for line in f_in]\n",
    "    wv = np.load(embeddings_path + '.npy')\n",
    "    model = {}\n",
    "    for i, w in enumerate(index2word):\n",
    "        model[w] = wv[i]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to find top similar words as defined by dot product\n",
    "Written by us :)\n",
    "\"\"\"\n",
    "def n_similar(inputVec, n, keySpace, model):\n",
    "    topWord = []\n",
    "    topDot = []\n",
    "    for i in range(0,n):\n",
    "        topWord.append('')\n",
    "        topDot.append(0)\n",
    "    length = inputVec.shape[0]\n",
    "    for key in keySpace:\n",
    "        lenKey = (np.reshape(model[key],(1,length)) @ np.reshape(model[key],(length,1))) ** .5\n",
    "        lenInput = (np.reshape(inputVec,(1,length)) @ np.reshape(inputVec,(length,1))) ** .5\n",
    "        dot = np.reshape(inputVec,(1,length)) @ np.reshape(model[key],(length,1)) / lenKey / lenInput\n",
    "        for j in range(0,n):\n",
    "            if (dot > topDot[n - j - 1]):\n",
    "                if (j != 0):\n",
    "                    topWord[n - j] = topWord[n - j - 1]\n",
    "                    topDot[n - j] = topDot[n - j - 1]\n",
    "                topWord[n - j - 1] = key\n",
    "                topDot[n - j - 1] = dot\n",
    "    return topWord, topDot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a2a667194663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"deps\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mconvert_to_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'converted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3a52d28d064e>\u001b[0m in \u001b[0;36mconvert_to_binary\u001b[0;34m(embedding_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".vocab\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvocab_write\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;34m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;34m\"\"\" Return the next decoded line from the input stream.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size, keepends)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# If size is given, we call read() only once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirstline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;31m# If we're at a \"\\r\" read one extra character (which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size, chars, firstline)\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0mnewdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                 \u001b[0mnewdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m             \u001b[0;31m# decode bytes (those remaining from the last call included)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytebuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " try:\n",
    "     path = \"deps\"\n",
    "     convert_to_binary(path)\n",
    "     print('converted')\n",
    "     x = load_embeddings_binary(path)\n",
    "     keys = x.keys()\n",
    " except FileNotFoundError:\n",
    "     raise Exception(f'FILE {path}.txt NOT FOUND PLEASE DOWNLOAD DEPENDENCY-BASED [WORDS] DATASET AND EXTRACT IN DIRECTORY FROM https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted\n"
     ]
    }
   ],
   "source": [
    "# note: the path refers to a <path>.txt file, you will need to update your extension\n",
    "path = \"deps\"\n",
    "convert_to_binary(path)\n",
    "print('converted')\n",
    "x = load_embeddings_binary(path)\n",
    "keys = x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['berry', 'blackberry', 'cantaloupe', 'mangosteen', 'apricot']\n",
      "[array([[0.8527363]]), array([[0.8527363]]), array([[0.77408645]]), array([[0.76526394]]), array([[0.76039084]])]\n"
     ]
    }
   ],
   "source": [
    "math = x['blackberry'] + x['berry']\n",
    "vals = keys\n",
    "result, length = n_similar(math, 5, vals, x)\n",
    "print(result)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvImport = np.loadtxt('spam.csv',delimiter=',', dtype = 'S100',encoding = 'latin-1', usecols = (0,1), skiprows = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxRow = np.shape(csvImport)[0]\n",
    "yTrain = np.zeros((maxRow,1))\n",
    "xTrain = np.zeros((maxRow,300))\n",
    "\n",
    "# get the average word vector\n",
    "for i in range(0,maxRow):\n",
    "    if (str(csvImport[i,0])[2:-1] == \"spam\"):\n",
    "        yTrain[i] = -1\n",
    "    else:\n",
    "        yTrain[i] = 1\n",
    "    \n",
    "    vecSum = np.zeros((300,))\n",
    "    wordCount = 0\n",
    "\n",
    "    words = str(csvImport[i,1])[2:-1].split(' ')\n",
    "    for word in words:\n",
    "        try:\n",
    "            word.translate(str.maketrans('', '', string.punctuation))\n",
    "            wordVec = x[word]\n",
    "            vecSum = wordVec + vecSum\n",
    "            wordCount = wordCount + 1\n",
    "        except:\n",
    "            pass\n",
    "    if (wordCount > 0):\n",
    "        avgWord = vecSum / wordCount\n",
    "    else:\n",
    "        avgWord - vecSum\n",
    "    xTrain[i,:] = np.reshape(avgWord,(1,300))\n",
    "# xTrain has row vectors of the average word value\n",
    "\n",
    "bagVocab = []\n",
    "\n",
    "# get vocab for the bag so it isn't too big\n",
    "for i in range(0,maxRow):\n",
    "    words = str(csvImport[i,1])[2:-1].split(' ')\n",
    "    for word in words:\n",
    "        word.translate(str.maketrans('', '', string.punctuation))\n",
    "        if (not word in bagVocab):\n",
    "            bagVocab.append(word)\n",
    "            \n",
    "xTrainBag = np.zeros((maxRow,len(bagVocab)))\n",
    "\n",
    "# get bag of word count vector\n",
    "for i in range(0,maxRow):\n",
    "    words = str(csvImport[i,1])[2:-1].split(' ')\n",
    "    for word in words:\n",
    "        word.translate(str.maketrans('', '', string.punctuation))\n",
    "        try:\n",
    "            index = bagVocab.index(word)\n",
    "            xTrainBag[i,index] = xTrainBag[i,index] + 1\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14347202295552366\n",
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14347202295552366\n",
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14347202295552366\n",
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14347202295552366\n",
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14347202295552366\n",
      "(4181, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14347202295552366\n",
      "(4181, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14347202295552366\n",
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14634146341463414\n",
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14634146341463414\n",
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14634146341463414\n",
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14634146341463414\n",
      "(4180, 300)\n",
      "[RIDGE] Best param: 0  error: 0.14634146341463414\n",
      "(4181, 300)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-242157245fa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mpseudoInv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0ms_matrix\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mVT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudoInv\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0ms_matrix\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parts = 8\n",
    "\n",
    "split_X = np.array_split(xTrain, parts)\n",
    "split_y = np.array_split(yTrain, parts)\n",
    "\n",
    "ridge_parameters = [0, 2**-1, 2**0, 2**1, 2**2, 2**3, 2**4]\n",
    "\n",
    "ridge_errors = []\n",
    "\n",
    "def calcErrorCount(expected, actual):\n",
    "    return (expected != actual).sum()\n",
    "\n",
    "def calcErrorRate(expected, actual):\n",
    "    return calcErrorCount(expected, actual) / len(actual)\n",
    "\n",
    "for errorEstimateIndex in range(0, parts):\n",
    "    for predictIndex in range(0, parts):\n",
    "        if errorEstimateIndex == predictIndex:\n",
    "            continue\n",
    "        \n",
    "#         print(f'Estimate: {errorEstimateIndex}  Predict: {predictIndex}')\n",
    "        \n",
    "        training_X = np.vstack(np.delete(split_X, [errorEstimateIndex, predictIndex], axis=0))\n",
    "        training_y = np.vstack(np.delete(split_y, [errorEstimateIndex, predictIndex], axis=0))\n",
    "        \n",
    "        print(np.shape(training_X))\n",
    "        \n",
    "        error_estimation_X = split_X[errorEstimateIndex]\n",
    "        error_estimation_y = split_y[errorEstimateIndex]\n",
    "        \n",
    "        predict_X = split_X[predictIndex]\n",
    "        predict_y = split_y[predictIndex]\n",
    "        \n",
    "        \"\"\"\n",
    "        Ridge Regression\n",
    "        \"\"\"\n",
    "        \n",
    "        # Estimate w for each choice of the regularization parameter\n",
    "        ridge_w = []\n",
    "        for param in ridge_parameters:\n",
    "            U,s,VT = np.linalg.svd(training_X,full_matrices=False)\n",
    "            s_matrix = np.identity(len(s)) * s;\n",
    "            V = VT.T\n",
    "\n",
    "            pseudoInv = np.linalg.inv((V @ s_matrix @ VT) + (np.identity(len(s)) * param))\n",
    "            \n",
    "            w = pseudoInv @ V @ s_matrix @ U.T @ training_y\n",
    "            \n",
    "            ridge_w.append(w)\n",
    "        \n",
    "        # Select the best value for the regularization parameter\n",
    "        # by estimating the error on the first holdout set\n",
    "        \n",
    "        leastErrorRateRidge = math.inf\n",
    "        leastErrorParamRidge = None\n",
    "        \n",
    "        for i in range(len(ridge_w)):\n",
    "            w = ridge_w[i]\n",
    "            estimated_y = np.sign(error_estimation_X @ w)\n",
    "            errorRate = calcErrorRate(error_estimation_y, estimated_y)\n",
    "            \n",
    "#             print(f'error for {ridge_parameters[i]} is {errorRate}')\n",
    "            \n",
    "            if errorRate < leastErrorRateRidge:\n",
    "                leastErrorRateRidge = errorRate\n",
    "                leastErrorParamRidge = ridge_parameters[i]\n",
    "        \n",
    "        print(f'[RIDGE] Best param: {leastErrorParamRidge}  error: {leastErrorRateRidge}')\n",
    "        \n",
    "        # Use the w corresponding to the best value of the regularization\n",
    "        # parameter to predict the labels of the remaining holdout set\n",
    "        \n",
    "        predicted_y = np.sign(predict_X @ w)\n",
    "        errorRate = calcErrorRate(predict_y, predicted_y)\n",
    "        ridge_errors.append(errorRate)\n",
    "\n",
    "print(f'Average Ridge Error Rate: {(np.mean(ridge_errors) * 100).round(3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4180, 11706)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (11706,11706) (4180,4180) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-ba781fc5be74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mpseudoInv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0ms_matrix\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mVT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudoInv\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0ms_matrix\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (11706,11706) (4180,4180) "
     ]
    }
   ],
   "source": [
    "parts = 8\n",
    "\n",
    "split_X = np.array_split(xTrainBag, parts)\n",
    "split_y = np.array_split(yTrain, parts)\n",
    "\n",
    "ridge_parameters = [0, 2**-1, 2**0, 2**1, 2**2, 2**3, 2**4]\n",
    "\n",
    "ridge_errors = []\n",
    "\n",
    "def calcErrorCount(expected, actual):\n",
    "    return (expected != actual).sum()\n",
    "\n",
    "def calcErrorRate(expected, actual):\n",
    "    return calcErrorCount(expected, actual) / len(actual)\n",
    "\n",
    "for errorEstimateIndex in range(0, parts):\n",
    "    for predictIndex in range(0, parts):\n",
    "        if errorEstimateIndex == predictIndex:\n",
    "            continue\n",
    "        \n",
    "#         print(f'Estimate: {errorEstimateIndex}  Predict: {predictIndex}')\n",
    "        \n",
    "        training_X = np.vstack(np.delete(split_X, [errorEstimateIndex, predictIndex], axis=0))\n",
    "        training_y = np.vstack(np.delete(split_y, [errorEstimateIndex, predictIndex], axis=0))\n",
    "        \n",
    "        print(np.shape(training_X))\n",
    "        \n",
    "        error_estimation_X = split_X[errorEstimateIndex]\n",
    "        error_estimation_y = split_y[errorEstimateIndex]\n",
    "        \n",
    "        predict_X = split_X[predictIndex]\n",
    "        predict_y = split_y[predictIndex]\n",
    "        \n",
    "        \"\"\"\n",
    "        Ridge Regression\n",
    "        \"\"\"\n",
    "        \n",
    "        # Estimate w for each choice of the regularization parameter\n",
    "        ridge_w = []\n",
    "        for param in ridge_parameters:\n",
    "            U,s,VT = np.linalg.svd(training_X,full_matrices=False)\n",
    "            s_matrix = np.identity(len(s)) * s;\n",
    "            V = VT.T\n",
    "\n",
    "            pseudoInv = np.linalg.inv((V @ s_matrix @ VT) + (np.identity(len(s)) * param))\n",
    "            \n",
    "            w = pseudoInv @ V @ s_matrix @ U.T @ training_y\n",
    "            \n",
    "            ridge_w.append(w)\n",
    "        \n",
    "        # Select the best value for the regularization parameter\n",
    "        # by estimating the error on the first holdout set\n",
    "        \n",
    "        leastErrorRateRidge = math.inf\n",
    "        leastErrorParamRidge = None\n",
    "        \n",
    "        for i in range(len(ridge_w)):\n",
    "            w = ridge_w[i]\n",
    "            estimated_y = np.sign(error_estimation_X @ w)\n",
    "            errorRate = calcErrorRate(error_estimation_y, estimated_y)\n",
    "            \n",
    "#             print(f'error for {ridge_parameters[i]} is {errorRate}')\n",
    "            \n",
    "            if errorRate < leastErrorRateRidge:\n",
    "                leastErrorRateRidge = errorRate\n",
    "                leastErrorParamRidge = ridge_parameters[i]\n",
    "        \n",
    "        print(f'[RIDGE] Best param: {leastErrorParamRidge}  error: {leastErrorRateRidge}')\n",
    "        \n",
    "        # Use the w corresponding to the best value of the regularization\n",
    "        # parameter to predict the labels of the remaining holdout set\n",
    "        \n",
    "        predicted_y = np.sign(predict_X @ w)\n",
    "        errorRate = calcErrorRate(predict_y, predicted_y)\n",
    "        ridge_errors.append(errorRate)\n",
    "\n",
    "print(f'Average Ridge Error Rate: {(np.mean(ridge_errors) * 100).round(3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
