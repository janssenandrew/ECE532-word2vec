{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "\"\"\"\n",
    "Using \"Dependency Based\" dataset from\n",
    "url: https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Code for loading bin file is from a blog post\n",
    "url: https://blog.ekbana.com/loading-glove-pre-trained-word-embedding-model-from-text-file-faster-5d3e8f2b8455\n",
    "\"\"\"\n",
    "def convert_to_binary(embedding_path):\n",
    "    \"\"\"\n",
    "    Here, it takes path to embedding text file provided by glove.\n",
    "    :param embedding_path: takes path of the embedding which is in text format or any format other than binary.\n",
    "    :return: a binary file of the given embeddings which takes a lot less time to load.\n",
    "    \"\"\"\n",
    "    f = codecs.open(embedding_path + \".txt\", 'r', encoding='utf-8')\n",
    "    wv = []\n",
    "    with codecs.open(embedding_path + \".vocab\", \"w\", encoding='utf-8') as vocab_write:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            if count == 0:\n",
    "                pass\n",
    "            else:\n",
    "                splitlines = line.split()\n",
    "                vocab_write.write(splitlines[0].strip())\n",
    "                vocab_write.write(\"\\n\")\n",
    "                wv.append([float(val) for val in splitlines[1:]])\n",
    "            count += 1\n",
    "    np.save(embedding_path + \".npy\", np.array(wv))\n",
    "    \n",
    "def load_embeddings_binary(embeddings_path):\n",
    "    \"\"\"\n",
    "    It loads embedding provided by glove which is saved as binary file. Loading of this model is\n",
    "    about  second faster than that of loading of txt glove file as model.\n",
    "    :param embeddings_path: path of glove file.\n",
    "    :return: glove model\n",
    "    \"\"\"\n",
    "    with codecs.open(embeddings_path + '.vocab', 'r', 'utf-8') as f_in:\n",
    "        index2word = [line.strip() for line in f_in]\n",
    "    wv = np.load(embeddings_path + '.npy')\n",
    "    model = {}\n",
    "    for i, w in enumerate(index2word):\n",
    "        model[w] = wv[i]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to find top similar words as defined by dot product\n",
    "Written by us :)\n",
    "\"\"\"\n",
    "def n_similar(inputVec, n, keySpace, model):\n",
    "    topWord = []\n",
    "    topDot = []\n",
    "    for i in range(0,n):\n",
    "        topWord.append('')\n",
    "        topDot.append(0)\n",
    "    length = inputVec.shape[0]\n",
    "    for key in keySpace:\n",
    "        lenKey = (np.reshape(model[key],(1,length)) @ np.reshape(model[key],(length,1))) ** .5\n",
    "        lenInput = (np.reshape(inputVec,(1,length)) @ np.reshape(inputVec,(length,1))) ** .5\n",
    "        dot = np.reshape(inputVec,(1,length)) @ np.reshape(model[key],(length,1)) / lenKey / lenInput\n",
    "        for j in range(0,n):\n",
    "            if (dot > topDot[n - j - 1]):\n",
    "                if (j != 0):\n",
    "                    topWord[n - j] = topWord[n - j - 1]\n",
    "                    topDot[n - j] = topDot[n - j - 1]\n",
    "                topWord[n - j - 1] = key\n",
    "                topDot[n - j - 1] = dot\n",
    "    return topWord, topDot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted\n"
     ]
    }
   ],
   "source": [
    "# note: the path refers to a <path>.txt file, you will need to update your extension\n",
    "path = \"deps\"\n",
    "convert_to_binary(path)\n",
    "print('converted')\n",
    "x = load_embeddings_binary(path)\n",
    "keys = x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soup', 'boing', 'workin', 'raving', 'goodnight']\n",
      "[array([[0.34218405]]), array([[0.2252225]]), array([[0.21887097]]), array([[0.2166584]]), array([[0.2162438]])]\n"
     ]
    }
   ],
   "source": [
    "math = x['king'] - x['man'] + x['woman']\n",
    "vals = keys\n",
    "result, length = n_similar(math, 5, vals, x)\n",
    "print(result)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
